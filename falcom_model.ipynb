{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f22434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using transformers_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b69c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True\n",
    "print(torch.version.cuda)  # Should print 11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a989a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894492b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(torch.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c69e6",
   "metadata": {},
   "source": [
    "model training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import time\n",
    "import psutil\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, TrainerCallback, EarlyStoppingCallback\n",
    "\n",
    "def close_file_handles(file_path):\n",
    "    try:\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'open_files']):\n",
    "            try:\n",
    "                open_files = proc.info.get('open_files')\n",
    "                if open_files is None:\n",
    "                    continue\n",
    "                for file in open_files:\n",
    "                    if file.path.startswith(file_path):\n",
    "                        print(f\"Closing handle: {file.path} (PID: {proc.pid}, Name: {proc.info['name']})\")\n",
    "                        proc.terminate()\n",
    "                        proc.wait(timeout=5)\n",
    "            except (psutil.AccessDenied, psutil.NoSuchProcess, Exception) as e:\n",
    "                print(f\"Skipping process {proc.pid}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error closing handles: {e}\")\n",
    "        \n",
    "# Check disk space\n",
    "total, used, free = shutil.disk_usage('D:/')\n",
    "if free < 5 * 1024**3:\n",
    "    raise RuntimeError(f\"Insufficient disk space: {free/1024**3:.2f}GB free, need ~5GB\")\n",
    "\n",
    "# Check write access\n",
    "output_dir = r'D:/Python/dating coach/dating_coach/dating_coach_gpt2'\n",
    "if not os.access(output_dir, os.W_OK):\n",
    "    raise RuntimeError(\"No write access to output directory\")\n",
    "\n",
    "# Set PyTorch memory optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Memory logging callback\n",
    "class MemoryLoggingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Step {state.global_step}: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Save checkpoint callback\n",
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        try:\n",
    "            model.save_pretrained(os.path.join(args.output_dir, f'checkpoint-{state.global_step}'))\n",
    "            tokenizer.save_pretrained(os.path.join(args.output_dir, f'checkpoint-{state.global_step}'))\n",
    "            print(f\"Saved checkpoint at step {state.global_step}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Checkpoint save failed at step {state.global_step}: {e}\")\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = str(text).strip()\n",
    "    if not text[-1] in [' ', '.', ',', '!', '?']:\n",
    "        words = text.split()\n",
    "        text = ' '.join(words[:-1]) if words else text\n",
    "    return text.replace(\"your doing\", \"you're doing\").replace(\"mistake guys make\", \"mistakes guys make\")\n",
    "\n",
    "# Read CSV data\n",
    "data = []\n",
    "csv_path = r'D:/Python/dating coach/formatted_data.csv'  # Adjust\n",
    "if os.path.isfile(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    data = [{'title': row['title'], 'text': clean_text(row['text'])} for _, row in df.iterrows()]\n",
    "else:\n",
    "    for file in os.listdir(csv_path):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(csv_path, file))\n",
    "            data.extend([{'title': row['title'], 'text': clean_text(row['text'])} for _, row in df.iterrows()])\n",
    "\n",
    "# Format samples\n",
    "formatted_samples = []\n",
    "for item in data:\n",
    "    sample = (\n",
    "        f\"[Scenario]: {item['title']}\\n\"\n",
    "        f\"{item['text']}\\n\"\n",
    "        f\"[Assistant]: For {item['title'].lower()}, keep it short and casual, like: 'Hey, great meeting you! Free this weekend?' Avoid long, eager messages as they can seem pushy.\"\n",
    "    )\n",
    "    formatted_samples.append(sample)\n",
    "\n",
    "# # Subsample\n",
    "# if len(formatted_samples) > 500:\n",
    "#     formatted_samples = formatted_samples[:500]\n",
    "dataset = Dataset.from_dict({'text': formatted_samples})\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "special_tokens = {'additional_special_tokens': ['[Scenario]', '[Assistant]']}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))  # Fix lm_head.weight\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenize dataset with labels\n",
    "def tokenize(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128  # Reduced for speed\n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = tokenized_dataset.shuffle().select(range(int(0.8 * len(tokenized_dataset))))\n",
    "eval_dataset = tokenized_dataset.shuffle().select(range(int(0.8 * len(tokenized_dataset)), len(tokenized_dataset)))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=60,  # Increased\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-6,  # Lowered\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    save_safetensors=False,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type='linear',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[\n",
    "        MemoryLoggingCallback(),\n",
    "        SaveCheckpointCallback(),\n",
    "        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    try:\n",
    "        close_file_handles(os.path.join(output_dir, 'final_new'))\n",
    "        model.save_pretrained(os.path.join(output_dir, 'final_new'), save_safetensors=False)\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, 'final_new'))\n",
    "    except Exception as save_e:\n",
    "        print(f\"Final save failed: {save_e}\")\n",
    "    raise\n",
    "\n",
    "# Save model and tokenizer\n",
    "for attempt in range(3):\n",
    "    try:\n",
    "        close_file_handles(os.path.join(output_dir, 'final_new'))\n",
    "        model.save_pretrained(os.path.join(output_dir, 'final_new'), save_safetensors=False)\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, 'final_new'))\n",
    "        print(\"Final model saved to final_new\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Final save attempt {attempt+1} failed: {e}\")\n",
    "        time.sleep(5)\n",
    "        if attempt == 2:\n",
    "            print(\"Final save failed after 3 attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize dataset with labels\n",
    "def tokenize(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128  # Reduced for speed\n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = tokenized_dataset.shuffle().select(range(int(0.8 * len(tokenized_dataset))))\n",
    "eval_dataset = tokenized_dataset.shuffle().select(range(int(0.8 * len(tokenized_dataset)), len(tokenized_dataset)))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=30,  # Increased\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,  # Lowered\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    save_safetensors=False,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type='linear',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[\n",
    "        MemoryLoggingCallback(),\n",
    "        SaveCheckpointCallback(),\n",
    "        EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.01)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    try:\n",
    "        close_file_handles(os.path.join(output_dir, 'final_new'))\n",
    "        model.save_pretrained(os.path.join(output_dir, 'final_new'), save_safetensors=False)\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, 'final_new'))\n",
    "    except Exception as save_e:\n",
    "        print(f\"Final save failed: {save_e}\")\n",
    "    raise\n",
    "\n",
    "# Save model and tokenizer\n",
    "for attempt in range(3):\n",
    "    try:\n",
    "        close_file_handles(os.path.join(output_dir, 'final_new'))\n",
    "        model.save_pretrained(os.path.join(output_dir, 'final_new'), save_safetensors=False)\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, 'final_new'))\n",
    "        print(\"Final model saved to final_new\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Final save attempt {attempt+1} failed: {e}\")\n",
    "        time.sleep(5)\n",
    "        if attempt == 2:\n",
    "            print(\"Final save failed after 3 attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total token count\n",
    "total_tokens = sum(len(input_ids) for input_ids in dataset['input_ids'])\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca127338",
   "metadata": {},
   "source": [
    "interupted training continue code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bf058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import psutil\n",
    "import time\n",
    "import numpy\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, TrainerCallback, EarlyStoppingCallback\n",
    "# Override torch.load for trusted checkpoints\n",
    "original_torch_load = torch.load\n",
    "def custom_torch_load(*args, **kwargs):\n",
    "    kwargs['weights_only'] = False  # Disable weights_only for trusted checkpoint\n",
    "    return original_torch_load(*args, **kwargs)\n",
    "torch.load = custom_torch_load\n",
    "# Close open file handles\n",
    "def close_file_handles(file_path):\n",
    "    try:\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'open_files']):\n",
    "            try:\n",
    "                open_files = proc.info.get('open_files')\n",
    "                if open_files is None:\n",
    "                    continue\n",
    "                for file in open_files:\n",
    "                    if file.path.startswith(file_path):\n",
    "                        print(f\"Closing handle: {file.path} (PID: {proc.pid}, Name: {proc.info['name']})\")\n",
    "                        proc.terminate()\n",
    "                        proc.wait(timeout=5)\n",
    "            except (psutil.AccessDenied, psutil.NoSuchProcess, Exception) as e:\n",
    "                print(f\"Skipping process {proc.pid}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error closing handles: {e}\")\n",
    "\n",
    "# Check disk space\n",
    "total, used, free = shutil.disk_usage('D:/')\n",
    "if free < 10 * 1024**3:\n",
    "    raise RuntimeError(f\"Insufficient disk space: {free/1024**3:.2f}GB free, need ~10GB\")\n",
    "\n",
    "# Check write access\n",
    "output_dir = r'D:/Python/dating coach/dating_coach/dating_coach_gpt2'\n",
    "if not os.access(output_dir, os.W_OK):\n",
    "    raise RuntimeError(\"No write access to output directory\")\n",
    "\n",
    "# Find latest checkpoint\n",
    "checkpoints = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "if not checkpoints:\n",
    "    raise RuntimeError(\"No checkpoints found. Must restart training.\")\n",
    "latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
    "checkpoint_path = os.path.join(output_dir, latest_checkpoint)\n",
    "print(f\"Resuming from: {checkpoint_path}\")\n",
    "\n",
    "# Set PyTorch memory optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Memory logging callback\n",
    "class MemoryLoggingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Step {state.global_step}: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Save checkpoint callback\n",
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                close_file_handles(checkpoint_dir)\n",
    "                model.save_pretrained(checkpoint_dir, save_safetensors=False)\n",
    "                tokenizer.save_pretrained(checkpoint_dir)\n",
    "                print(f\"Saved checkpoint at step {state.global_step}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Checkpoint save attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(5)\n",
    "                if attempt == 2:\n",
    "                    print(f\"Checkpoint save failed at step {state.global_step}\")\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = str(text).strip()\n",
    "    if not text[-1] in [' ', '.', ',', '!', '?']:\n",
    "        words = text.split()\n",
    "        text = ' '.join(words[:-1]) if words else text\n",
    "    return text.replace(\"your doing\", \"you're doing\").replace(\"mistake guys make\", \"mistakes guys make\")\n",
    "\n",
    "# Read CSV data\n",
    "data = []\n",
    "csv_path = r'D:/Python/dating coach/formatted_data.csv'  # Adjust\n",
    "if os.path.isfile(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    data = [{'title': row['title'], 'text': clean_text(row['text'])} for _, row in df.iterrows()]\n",
    "else:\n",
    "    for file in os.listdir(csv_path):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(csv_path, file))\n",
    "            data.extend([{'title': row['title'], 'text': clean_text(row['text'])} for _, row in df.iterrows()])\n",
    "\n",
    "# Format samples with varied responses\n",
    "responses = [\n",
    "    \"Try a concise approach: 'Hey, loved our chat! Free for coffee this weekend?' Avoid over-texting.\",\n",
    "    \"Wait 1–2 days, then send a light message like: 'Hey, how’s it going?' Don’t sound too eager.\",\n",
    "    \"Keep it playful: 'Hey, still thinking about that great convo! Up for a drink?' Avoid long texts.\"\n",
    "]\n",
    "import random\n",
    "formatted_samples = []\n",
    "for item in data:\n",
    "    title = item['title'].lower()\n",
    "    sample = (\n",
    "        f\"[Scenario]: {item['title']}\\n\"\n",
    "        f\"{item['text']}\\n\"\n",
    "        f\"[Assistant]: For {title}, {random.choice(responses)}\"\n",
    "    )\n",
    "    formatted_samples.append(sample)\n",
    "\n",
    "# Use full dataset\n",
    "dataset = Dataset.from_dict({'text': formatted_samples})\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "except Exception as e:\n",
    "    print(f\"Tokenizer load failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load model from checkpoint\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
    "    model.gradient_checkpointing_enable()\n",
    "except Exception as e:\n",
    "    print(f\"Model load failed: {e}\")\n",
    "    raise\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = tokenized_dataset.shuffle().select(range(int(0.8 * len(tokenized_dataset))))\n",
    "eval_dataset = tokenized_dataset.shuffle().select(range(int(0.8 * len(tokenized_dataset)), len(tokenized_dataset)))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    save_safetensors=False,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type='linear',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[\n",
    "        MemoryLoggingCallback(),\n",
    "        SaveCheckpointCallback(),\n",
    "        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    try:\n",
    "        close_file_handles(os.path.join(output_dir, 'final_new'))\n",
    "        model.save_pretrained(os.path.join(output_dir, 'final_new'), save_safetensors=False)\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, 'final_new'))\n",
    "    except Exception as save_e:\n",
    "        print(f\"Final save failed: {save_e}\")\n",
    "    raise\n",
    "\n",
    "# Save final model\n",
    "for attempt in range(3):\n",
    "    try:\n",
    "        close_file_handles(os.path.join(output_dir, 'final_new'))\n",
    "        model.save_pretrained(os.path.join(output_dir, 'final_new'), save_safetensors=False)\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, 'final_new'))\n",
    "        print(\"Final model saved to final_new\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Final save attempt {attempt+1} failed: {e}\")\n",
    "        time.sleep(5)\n",
    "        if attempt == 2:\n",
    "            print(\"Final save failed after 3 attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeec429",
   "metadata": {},
   "source": [
    "Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0')\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(r'D:/Python/dating coach/dating_coach/dating_coach_gpt2/final_new')\n",
    "except Exception as e:\n",
    "    print(f\"Tokenizer load failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(r'D:/Python/dating coach/dating_coach/dating_coach_gpt2/final_new')\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.eval()  # Set to evaluation mode\n",
    "except Exception as e:\n",
    "    print(f\"Model load failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Query\n",
    "query = \"User: What advice do you have for mistakes guys make after getting her number? [Scenario]: Mistakes Guys Make After Getting Her Number\"\n",
    "\n",
    "# Tokenize and move inputs to GPU\n",
    "try:\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move all tensors to cuda:0\n",
    "\n",
    "    # # Verify device\n",
    "    # for k, v in inputs.items():\n",
    "    #     print(f\"{k} device: {v.device}\")\n",
    "    # print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            top_k=50, \n",
    "            repetition_penalty=2,\n",
    "            stop_sequences=['[Assistant]', '.']\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    cleaned_response = re.sub(r'\\[.*?\\]', '', response).strip()\n",
    "    print(f\"Response: {cleaned_response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Inference failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def clean_response(text):\n",
    "    # Remove special tokens and fix framing\n",
    "    text = re.sub(r'\\[.*?\\]', '', text).strip()  # Remove [Scenario], [Assistant]\n",
    "    text = re.sub(r'<\\|endoftext\\|>', '', text).strip()\n",
    "    # Split into sentences, ensure complete\n",
    "    sentences = [s.strip() for s in text.split('. ') if s.strip()]\n",
    "    if sentences and not sentences[-1].endswith('.'):\n",
    "        sentences[-1] += '.'\n",
    "    # Ensure solution-focused\n",
    "    if not any(word in text.lower() for word in ['avoid', 'suggest', 'try', 'action']):\n",
    "        sentences.append(\"To address this, focus on clear communication and positive engagement.\")\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = 'D:/Python/dating coach/dating_coach/dating_coach_gpt2/final_new'  # Adjust\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"Load failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Detailed prompt\n",
    "query = (\n",
    "    '''Mistakes Guys Make After Getting female number and provide Common mistakes include texting too much too soon, being overly eager, or not following up quickly enough.\n",
    "    Provide a detailed, well-structured response with practical advice to address the scenario. Explain how to avoid each mistake and suggest specific actions. Ensure the response is positive, concise, and ends with a complete sentence.'''\n",
    ")\n",
    "\n",
    "try:\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,  # Longer output\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,  # Diverse\n",
    "            temperature=1.0,  # Creative\n",
    "            repetition_penalty=2.0,  # Avoid repetition\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id  # Stop at end\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    cleaned_response = clean_response(response)\n",
    "    print(f\"Response: {cleaned_response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Inference failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def clean_response(text):\n",
    "    # Remove special tokens and irrelevant phrases\n",
    "    text = re.sub(r'\\[.*?\\]', '', text).strip()  # Remove [Scenario], [Assistant]\n",
    "    text = re.sub(r'<\\|endoftext\\|>', '', text).strip()\n",
    "    # Remove off-topic phrases (e.g., \"bhaiya\", \"good night\")\n",
    "    irrelevant = r'\\b(bhaiya|yaar|bro|good night|welcome back|thank god|complaints)\\b'\n",
    "    text = re.sub(irrelevant, '', text, flags=re.IGNORECASE).strip()\n",
    "    # Split into sentences, ensure complete\n",
    "    sentences = [s.strip() for s in text.split('. ') if s.strip()]\n",
    "    if sentences and not sentences[-1].endswith('.'):\n",
    "        sentences[-1] += '.'\n",
    "    # Ensure scenario-specific advice\n",
    "    required_phrases = ['texting too much', 'overly eager', 'following up']\n",
    "    if not any(phrase in text.lower() for phrase in required_phrases):\n",
    "        sentences.append(\n",
    "            \"To avoid texting too much, send one casual message within 24 hours. \"\n",
    "            \"Prevent being overly eager by matching her texting pace. \"\n",
    "            \"Follow up within 1–2 days to show interest.\"\n",
    "        )\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = 'D:/Python/dating coach/dating_coach/dating_coach_gpt2/final_new'  # Adjust\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"Load failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Stricter prompt\n",
    "query = (\n",
    "    '''Mistakes Guys Make After Getting female number and provide Common mistakes include texting too much too soon, being overly eager, or not following up quickly enough.\n",
    "    Provide a detailed, well-structured response with practical advice to address the scenario. Explain how to avoid each mistake and suggest specific actions. Ensure the response is positive, concise, and ends with a complete sentence.'''\n",
    ")\n",
    "\n",
    "try:\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,  # Allow more detail\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.9,  # Less rambling\n",
    "            repetition_penalty=2.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    cleaned_response = clean_response(response)\n",
    "    print(f\"Response: {cleaned_response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Inference failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def clean_response(text):\n",
    "    text = re.sub(r'\\[.*?\\]', '', text).strip()\n",
    "    text = re.sub(r'<\\|endoftext\\|>', '', text).strip()\n",
    "    irrelevant = r'\\b(bhaiya|yaar|bro|good night|welcome back|thank god|complaints|subscribe|social media|channel|followers|mentorship)\\b'\n",
    "    text = re.sub(irrelevant, '', text, flags=re.IGNORECASE).strip()\n",
    "    sentences = [s.strip() for s in text.split('. ') if s.strip()]\n",
    "    if sentences and not sentences[-1].endswith('.'):\n",
    "        sentences[-1] += '.'\n",
    "    required_phrases = ['texting too much', 'overly eager', 'following up']\n",
    "    response_lower = text.lower()\n",
    "    missing_phrases = [phrase for phrase in required_phrases if phrase not in response_lower]\n",
    "    if missing_phrases:\n",
    "        \n",
    "        additions = []\n",
    "        if 'texting too much' in missing_phrases:\n",
    "            additions.append(\"To avoid texting too much, send one casual message within 24 hours, like: 'Hey, great meeting you! Free for coffee?'\")\n",
    "        if 'overly eager' in missing_phrases:\n",
    "            additions.append(\"Prevent being overly eager by matching her texting pace with one text daily, such as: 'Hey, loved our chat! What’s up?'\")\n",
    "        if 'following up' in missing_phrases:\n",
    "            additions.append(\"Follow up within 1–2 days to show interest, e.g., 'Hey, how’s it going? Had fun talking!'\")\n",
    "        sentences.extend(additions)\n",
    "    negative = r'\\b(negative|stuck|struggling|reject|pressure|less interested)\\b'\n",
    "    text = re.sub(negative, 'positive', text, flags=re.IGNORECASE)\n",
    "    return '\\n'.join([f\"- {s}\" for s in sentences if s]) + '\\nThese steps foster a positive connection.'\n",
    "\n",
    "app = FastAPI()\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(r'D:/Python/dating_coach/d_coach/dating_coach_gpt2/final_new')\n",
    "    model = AutoModelForCausalLM.from_pretrained(r'D:/Python/dating_coach/d_coach/dating_coach_gpt2/final_new')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"Model load failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(query: str):\n",
    "    try:\n",
    "        prompt = (\n",
    "            f\"{query}\\n\"\n",
    "            f\"Provide a detailed, well-structured response addressing each mistake. Use bullet points for clarity. For each mistake: explain how to avoid it, suggest a specific action (e.g., example text message), and maintain a positive tone. Ensure the response is concise, ends with a complete sentence, and avoids irrelevant topics like social media or unrelated activities.\"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.8,\n",
    "                repetition_penalty=2.0,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        cleaned_response = clean_response(response)\n",
    "        return {\"response\": cleaned_response}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1eaed1",
   "metadata": {},
   "source": [
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_response(text, query):\n",
    "    text = re.sub(re.escape(query), '', text, flags=re.IGNORECASE).strip()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text).strip()\n",
    "    text = re.sub(r'<\\|endoftext\\|>', '', text).strip()\n",
    "    irrelevant = r'\\b(bhaiya|yaar|bro|good night|welcome back|thank god|complaints|subscribe|social media|channel|followers|mentorship|geekhoopermusic|eyeembrace|blueprint|clicking here|online dating)\\b'\n",
    "    text = re.sub(irrelevant, '', text, flags=re.IGNORECASE).strip()\n",
    "    sentences = [s.strip() for s in text.split('. ') if s.strip()]\n",
    "    if sentences and not sentences[-1].endswith('.'):\n",
    "        sentences[-1] += '.'\n",
    "    required_phrases = ['texting too much', 'overly eager', 'following up']\n",
    "    response_lower = text.lower()\n",
    "    missing_phrases = [phrase for phrase in required_phrases if phrase not in response_lower]\n",
    "    if missing_phrases or not sentences:\n",
    "        additions = []\n",
    "        if 'texting too much' in missing_phrases or not sentences:\n",
    "            additions.append(\"To avoid texting too much, send one casual message within 24 hours, like: 'Hey, great meeting you! Free for coffee?'\")\n",
    "        if 'overly eager' in missing_phrases or not sentences:\n",
    "            additions.append(\"Prevent being overly eager by matching her texting pace with one text daily, such as: 'Hey, loved our chat! What’s up?'\")\n",
    "        if 'following up' in missing_phrases or not sentences:\n",
    "            additions.append(\"Follow up within 1–2 days to show interest, e.g., 'Hey, how’s it going? Had fun talking!'\")\n",
    "        sentences = additions\n",
    "    negative = r'\\b(negative|stuck|struggling|reject|pressure|less interested)\\b'\n",
    "    text = re.sub(negative, 'positive', text, flags=re.IGNORECASE)\n",
    "    return '\\n'.join([f\"- {s}\" for s in sentences if s]) + '\\nThese actions foster a positive connection.'\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model_path = 'D:/Python/dating_coach/d_coach/dating_coach_gpt2/final_new'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device).eval()\n",
    "\n",
    "# Load dataset\n",
    "data = []\n",
    "csv_path = r'D:/Python/dating_coach/d_coach/data.csv'\n",
    "if os.path.isfile(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    data = [{'title': row['title'], 'text': str(row['text'])} for _, row in df.iterrows()]\n",
    "documents = [f\"{item['title']}: {item['text']}\" for item in data]\n",
    "\n",
    "# Embed documents\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedder.encode(documents, convert_to_numpy=True)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Query\n",
    "query = \"Mistakes Guys Make After Getting Her Number\\nCommon mistakes include texting too much too soon, being overly eager, or not following up quickly enough.\"\n",
    "query_embedding = embedder.encode([query])\n",
    "D, I = index.search(query_embedding, k=3)\n",
    "context = \"\\n\".join([documents[i] for i in I[0]])\n",
    "\n",
    "# Generate\n",
    "prompt = (\n",
    "    f\"Context: {context}\\n\"\n",
    "    f\"{query}\\n\"\n",
    "    f\"Respond with a concise, positive, and well-structured answer using bullet points. Address each mistake (texting too much, being overly eager, not following up) with an explanation of how to avoid it and a specific action (e.g., example text message). Avoid irrelevant topics like social media or unrelated activities.\"\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=256).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.6,\n",
    "        repetition_penalty=2.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "cleaned_response = clean_response(response, query)\n",
    "print(f\"Response:\\n{cleaned_response}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    local_files_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a11172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check available RAM before\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Before loading: {mem.available / (1024 ** 3):.2f} GB free\")\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# Load model (change model path)\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                            model_path, \n",
    "                                            quantization_config=bnb_config,  # if supported\n",
    "                                            device_map=\"auto\", \n",
    "                                            local_files_only=True\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Check available RAM after\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"After loading: {mem.available / (1024 ** 3):.2f} GB free\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def resource_monitor(ram_threshold=90, cpu_threshold=90):\n",
    "    while True:\n",
    "        mem = psutil.virtual_memory()\n",
    "        cpu = psutil.cpu_percent(interval=1)\n",
    "        if mem.percent >= ram_threshold or cpu >= cpu_threshold:\n",
    "            raise RuntimeError(\n",
    "                f\"ERROR: Resource usage exceeded threshold!\\n\"\n",
    "                f\"RAM: {mem.percent:.2f}% used ({mem.used / (1024 ** 3):.2f} GB / {mem.total / (1024 ** 3):.2f} GB)\\n\"\n",
    "                f\"CPU: {cpu:.2f}% used\"\n",
    "            )\n",
    "        time.sleep(2)  # Check every 2 seconds\n",
    "\n",
    "# Start resource monitor in background\n",
    "monitor_thread = threading.Thread(target=resource_monitor, args=(90, 90), daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # critical fix!\n",
    ")\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    quantization_config=bnb_config,  # if supported\n",
    "    device_map=\"auto\", \n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b23c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
